---
title: "Mathematical Statistics"
author: "林嘉文"
date: "2017年8月26日"
output: html_document
---
首先我们明确我们所学的是参数统计，与之相对应的是非参数统计。它们都会涉及到*参数估计*和*假定检验*，这也是数理统计要做的两件最主要的事情。参数统计与非参数统计的区别就在于，参数统计对于随机变量的分布是有假设的，而非参数统计则相对来说缺少对于分布的假设,相对更robust。

但是我们肯定想问，到底我们是在干什么呢？统计所在做的分为描述性统计和推断性统计(describetive statistics and inference statistics),我们现在说的是推断性统计到底在做什么。推断性统计其实都是为了研究“某个事情的规律”，这里的“某个事情”泛指某个随机变量，“规律”指的就是随机变量的分布。这世间很多东西可以简化为随机变量的各种取值，因此我们最想知道的就是这个随机变量的分布，而推断性统计就是为了将这个随机变量的分布找出来。对于随机变量而言，得到分布就得到了一切。

但是我们永远无法确切知道分布到底是什么样子的，因此我们只能*估计*，估计完了之后还要*检定*这个估计是否正确。



#Chapter 5 Parameter Estimation

* 样本必须iid，并具有代表性。
* 从大量数据中得出分布，有两种方法，一种是参数估计方法，一种是非参数估计方法，分别对应于参数统计和非参数统计。

##参数估计基本介绍

参数估计方法就是我们假设随机变量服从某个分布$f_{X}(x)=f(x,\theta)$,其中$f_{X}()$是已知函数而参数$\theta$是未知的，从而我们的任务从估计其分布变成估计参数，这里的$\theta$是一个概括，它可以是一个或者多个未知参数。

> 如果$f_{X}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$,则$\theta=(\mu,\sigma)$  
> 从这里我们就要记住我们现在的任务是估计一个*固定的*、*未知的*值。

##统计量

定义：$X^{n}=(X_{1},X_{2},X_{3},...,X_{n})$是从总体中取出的n个iid随机变量。$T(X^{n})=T(X_{1},X_{2},X_{3},...,X_{n})$则称为一个*统计量(statistic)*

> 统计量也是一个随机变量，因为它实际上是对$X^{n}$加了一个法则。因此统计量也具有概率分布，也有期望、方差这些性质。  
> 统计量中不能出现未知参数，不然不叫统计量。统计量的分布有可能已知有可能未知。与之相对应的一个东西叫做*枢轴量*，枢轴量是含有未知参数的随机变量，但是它的分布是已知的。  
> 为什么要弄这么一个统计量呢？在我看来，其实统计量就是用来估计参数的。它也符合我们的一个逻辑：我们只拥有许多数据，这些数据是iid样本$X^{n}$的一次（或多次）取值，那么我们构造的统计量也能得到相应的取值，如果这些统计量被证明是一些参数的好的估计，那么我们就可以用它们来估计参数了。  
> eg:$\bar{X_{n}}=\frac{\sum_{i=1}^{n}X_{i}}{n}$,$S_{n}^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}}{n-1}$,它们分别是样本均值和样本方差，但是实际上它们就是统计量。它们是由iii样本$X^{n}$构成的函数，它们是随机变量。


**样本均值$\bar{X_{n}}$**

样本均值，就是对样本取平均。
$$\bar{X_{n}}=\frac{1}{n}\sum\limits_{i=1}^{n}X_{i}$$
那么我们就会自然想知道它的期望、方差以及分布是什么，
$$E(\bar{X_{n}})=E(\frac{1}{n}\sum\limits_{i=1}^{n}X_{i})=\frac{1}{n}\sum\limits_{i=1}^{n}E(X_{i})=\frac{1}{n}\sum\limits_{i=1}^{n}\mu=\mu$$

> 因为$X_{i}$是iid的，所以得到这个结果。可以看到样本均值的期望就是$X_{i}$的期望$\mu$。因此我们把样本均值称为是总体均值$\mu$的无偏估计。

$$Var(\bar{X_{n}})=Var(\frac{1}{n}\sum\limits_{i=1}^{n}X_{i})=\frac{1}{n^{2}}Var(\sum\limits_{i=1}^{n}X_{i})=\frac{1}{n^{2}}\sum\limits_{i=1}^{n}Var(X_{i})=\frac{\sigma^{2}}{n}$$

> 求解过程中，因为$X_{i}$是独立同分布的，因此所有的协方差均为0。可以看到最后样本均值的方差和$X_{i}$的方差不同，它衡量的是$\bar{X_{n}}$这一随机变量的波动程度。  
> 当n越大，$Var(\bar{X_{n}})$越小。当$n\rightarrow\infty$，$Var(\bar{X_{n}})\rightarrow 0$,换句话说就是当n趋向无穷时，样本均值就是总体均值。


Theorem:suppose $X^{n}=(X_{1},X_{2},...X_{n})$是iid正态分布的随机样本，均值为$\mu$，方差为$\sigma^{2}$，那么就有
$$\bar{X_{n}}\sim N(\mu,\frac{\sigma^{2}}{n})$$

> 这个其实都不能算是一个theorem，因为这个实际上就是我们在概率论中说的sum of independent R.V.的结论的使用。

Central Limit Theorem:$X^{n}=(X_{1},X_{x},...,X_{n})$是iid的随机样本，均值为$\mu$，方差为$\sigma^{2}$，$\bar{X_{n}}=\frac{1}{b}\sum\limits_{i=1}^{n}X_{i}$,则
$$Z_{n}=\frac{\bar{X_{n}}-\mu}{\frac{\sigma}{\sqrt{n}}}\xrightarrow{d}N(0,1)$$

> 中心极限定理并不要求$X_{i}$的任何分布信息，只需要n足够大。

**样本方差$S_{n}^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}$**

我们的第一反应其实是仿照样本均值，分母似乎是n而不是(n-1)，但是我们可以证明分母是(n-1)才能使其是总体方差的无偏估计。

$$S_{n}^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}[(X_{i}-\mu)-(\bar{X_{n}}-\mu)]^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}[(X_{i}-\mu)^{2}-2(X_{i}-\mu)(\bar{X_{n}}-\mu)+(\bar{X_{n}}-\mu)^{2}]$$
$$\Rightarrow S_{n}^{2}=\frac{1}{n-1}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-2\sum\limits_{i=1}^{n}(X_{i}-\mu)(\bar{X_{n}}-\mu)+\sum\limits_{i=1}^{n}(\bar{X_{n}}-\mu)^{2}]=\frac{1}{n-1}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-2(\bar{X_{n}}-\mu)\sum\limits_{i=1}^{n}(X_{i}-\mu)+n\cdot(\bar{X_{n}}-\mu)^{2}]$$
$$\Rightarrow S_{n}^{2}=\frac{1}{n-1}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-2(\bar{X_{n}}-\mu)(\sum\limits_{i=1}^{n}X_{i}-n\cdot\mu)+n\cdot(\bar{X_{n}}-\mu)^{2}]=\frac{1}{n-1}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-2(\bar{X_{n}}-\mu)\cdot n\cdot(\bar{X_{n}}-\mu)+n\cdot(\bar{X_{n}}-\mu)^{2}]$$
$$\Rightarrow S_{n}^{2}=\frac{1}{n-1}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-n\cdot(\bar{X_{n}}-\mu)^{2}]$$
$$\therefore E(S_{n}^{2})=\frac{1}{n-1}E[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-n\cdot(\bar{X_{n}}-\mu)^{2}]=\frac{1}{n-1}(n\cdot\sigma^{2}-n\cdot\frac{\sigma^{2}}{n})=\sigma^{2}$$

> 显然我们可以看到，如果分母不是(n-1)，那么样本方差的期望就不是$\sigma^{2}$啦！

为了推出样本方差的方差和分布，我们需要证明：
$$\textrm{Suppose }X^{n}\textrm{ is a iid }N(\mu,\sigma^{2})\textrm{ random sample.Then for any n>1},S_{n}^{2}\,\,and\,\,\bar{X_{n}}\textrm{ are mutually independent.That is }S_{n}^{2}\bot\bar{X_{n}}$$

> 这个我们就不详细证了，我们说一下大致的思路。等价于要证明$(X_{1}-\bar{X_{n}},X_{2}-\bar{X_{n}},...,X_{n}-\bar{X_{n}})\bot X_{n}$,则我们进行一次换元，以简化字符，然后通过$f_{X,Y}(x,y)=h(x)\cdot g(y)$这个推论来证明独立。  
> 我们会用到雅可比矩阵的知识，因为我们需要在换元之后得到新的联合分布。  
> 这个理论要求X必须来自正态分布，在证明中要用到。所以一定要注意这个前提。

Theorem:$\frac{(n-1)S_{n}^{2}}{\sigma^{2}}\sim \chi_{n-1}^{2}$

需要说明的是，首先要求X来自一个正态分布，那么这个随机样本的样本方差有这样一个结论，其中$X_{n-1}^{2}$是自由度为(n-1)的卡方分布。

$$\frac{(n-1)S_{n}^{2}}{\sigma^{2}}=\sum\limits_{i=1}^{n}\frac{(X_{i}-\bar{X_{n}})^{2}}{\sigma^{2}}=\sum\limits_{i=1}^{n}\frac{[(X_{i}-\mu)-(\bar{X_{n}}-\mu)]^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}[\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}-n\cdot(\bar{X_{n}}-\mu)^{2}]$$
$$\Rightarrow \frac{(n-1)S_{n}^{2}}{\sigma^{2}}=\sum\limits_{i=1}^{n}(\frac{X_{i}-\mu}{\sigma})^{2}-n\cdot (\frac{\bar{X_{n}}-\mu}{\sigma})^{2}=\sum\limits_{i=1}^{n}(\frac{X_{i}-\mu}{\sigma})^{2}-(\frac{\bar{X_{n}}-\mu}{\frac{\sigma}{\sqrt{n}}})^{2}$$
$$\Rightarrow \textrm{令 }U=\sum\limits_{i=1}^{n}(\frac{X_{i}-\mu}{\sigma})^{2},Z=(\frac{\bar{X_{n}}-\mu}{\frac{\sigma}{\sqrt{n}}})^{2},W=\frac{(n-1)S_{n}^{2}}{\sigma^{2}}$$
$$\therefore U=Z+W$$
$$M_{U}(t)=M_{Z+W}(t)=E[e^{(z+w)t}]=E(e^{zt+wt})=\int\int e^{zt+wt}f_{Z,W}(z,w)dzdw=\int e^{zt}f(z)dz\int e^{wt}f(w)dw=(1-2t)^{-\frac{1}{2}}M_{W}(t)$$
$$\because M_{U}(t)=(1-2t)^{-\frac{n}{2}}\quad\therefore M_{W}(t)=(1-2t)^{-\frac{n-1}{2}}$$
$$\therefore \frac{(n-1)S_{n}^{2}}{\sigma^{2}}\sim \chi_{n-1}^{2}$$

> 在我们使用矩母函数的时候，我们把联合分布拆开就用到了样本均值和样本方差独立的条件。  
> $If\,\,X\sim N(0,1),then\,\,X^{2}\sim \chi_{1}^{2}$  
> $If\,\,X_{i}\sim N(0,1),iid,then\,\,\sum_{i=1}^{n}X_{i}^{2}\sim \chi_{n}^{2}$  
> 为什么这里自由度是(n-1)呢，其实也是有讲究的。假设有$W_{n}^{2}=\frac{1}{n-1}\sum\limits_{i=1}^{n}(X_{i}-\mu)^{2}$,则有$\frac{(n-1)W_{n}^{2}}{\sigma^{2}}=\sum\limits_{i=1}^{n}(\frac{X_{i}-\mu}{\sigma})^{2}\sim \chi_{n}^{2}$。但是因为我们在$S_{n}^{2}$里面把均值换成了样本均值，就会丢失了一个信息，所以自由度减少1.

有了分布之后，方差就很好计算了。因为

$$E(\chi_{n-1}^{2})=n-1,Var(\chi_{n-1}^{2})=2(n-1)$$
$$\Rightarrow Var(\frac{(n-1)S_{n}^{2}}{\sigma^{2}})=2(n-1)$$
$$\Rightarrow \frac{(n-1)^{2}}{\sigma^{4}}Var(S_{n}^{2})=2(n-1)$$
$$\therefore Var(S_{n}^{2})=\frac{2\sigma^{4}}{n-1}$$


##两大估计方法

前面我们讲的样本均值和样本方差实际上也可以作为均值和方差的估计量，但是它们是我们从直觉中找到的统计量，而不是通过某种方法找到的。现在我们要介绍两种最主要的参数估计方法。

**Maximum likelihood estimation**

*本质：观察到已经发生的事件，求得使这件事发生概率最大的参数的值。*

> 这种方法有一个很本质的想法，比如说一个池塘有A,B两种鱼，你随便捞10条鱼，其中8条是A，2条是B，从而你估计池塘中80%是A鱼;又比如说拿一枚硬币投掷30次，结果是20次为字朝上，10次花朝上，从而你估计字朝上的概率是$\frac{2}{3}$。  
> 概率论是已知参数求概率，而数理统计是由观察到的数据估计参数。

The procedure to find a maximum likelihood estimator for $\theta$

1. 得到$L(\theta)$,$L(\theta)=\prod\limits_{i=1}^{n}f_{Y}(yi,\theta)$.
2. 对$L(\theta)$取对数,$\rho(\theta)=ln(L(\theta))$（方便运算，尤其对于指数类的概率分布函数。并且ln是单调递增的，不会影响到$L(\theta)$的极值点）
3. 解等式$\frac{d\rho(\theta)}{d\theta}=0$，求得极值点
4. 验证二阶导是否小于0，保证极值点为极大值点。（这一步一般来说不需要）

> $L(\theta)被称为似然函数(likelihood function),要得到这个东西，前提就是我们已知Y的分布，所以最大似然估计很显然就是参数估计的方法。$  
> 实际上我们在求的东西是，$\hat{\theta}=arg\,max_{\theta}L(\theta|Y_{1},Y_{2},...Y_{n})$。在随机样本没有取值之前，这个表达式是一个随机变量，也就是一个estimator，但是当随机样本取值之后，这个estimator也会取一个值，也就是estimate。

> eg1:我们以掷硬币为例子，我们想要估计的很显然就是伯努利分布中的那个p。  
> 对于伯努利分布，$f_{X}(x)=p^{x}(1-p)^{1-x},x\in\{0,1\}$  
> $\Rightarrow L(p)=f_{X}(x_{1})f_{X}(x_{1})\cdots f_{X}(x_{n})=p^{\sum\limits_{i=1}^{n}x_{i}}(1-p)^{n-\sum\limits_{i=1}^{n}x_{i}}$  
> $\Rightarrow\rho(p)=ln(L(p))=\sum\limits_{i=1}^{n}x_{i}ln(p)+(n-\sum\limits_{i=1}^{n}x_{i})ln(1-p)$  
> $\Rightarrow\frac{d\rho(p)}{dp}=\sum\limits_{i=1}^{n}x_{i}\frac{1}{p}-(n-\sum\limits_{i=1}^{n}x_{i})\frac{1}{1-p}=\frac{(1-p)\sum\limits_{i=1}^{n}x_{i}-p(n-\sum\limits_{i=1}^{n}x_{i})}{p(1-p)}=0$  
> $\therefore\quad \hat{p}=\frac{1}{n}\sum\limits_{i=1}^{n}x_{i}$  
> 这个就是p的估计量，它首先是一个随机变量，其实就是样本均值。当我们进行实验得到数据之后，就可以得到一个具体的估计值。

> eg2:当未知参数不止一个的时候，例如有两个参数则要对两个参数求偏导并联立求解。$X_{i}\sim N(\mu,\sigma),X^{n}=(X_{1},X_{2},...,X_{n})$。求均值和方差的最大似然估计量。  
> $f_{X}(X)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-frac{(x-\mu)^{2}}{2\sigma^{2}}}$  
> $\Rightarrow L(\mu,\sigma^{2})=\prod\limits_{i=1}^{n}f_{X}(x_{i},\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{\frac{n}{2}}}e^{-\frac{\sum\limits_{i=1}^{n}(x_{i}-\mu)^{2}}{2\sigma^{2}}}$  
> $\Rightarrow \rho(\mu,\sigma^{2})=ln(L(\mu,\sigma^{2}))=-\frac{n}{2}ln(2\pi\sigma^{2})-\frac{\sum\limits_{i=1}^{n}(x_{i}-\mu)^{2}}{2\sigma^{2}}$  
> $$\left\{
\begin{array}{1}
\frac{d\rho(\mu,\sigma^{2})}{d\mu}=\frac{\sum\limits_{i=1}^{n}(x_{i}-\mu)}{\sigma^{2}}=0\\ \frac{d\rho(\mu,\sigma^{2})}{d\sigma^{2}}=-\frac{n}{2}\cdot\frac{1}{\sigma^{2}}+\frac{\sum\limits_{i=1}^{n}(x_{i}-\mu)^{2}}{2}\cdot\frac{1}{\sigma^{4}}=0\\
\end{array}
\right.$$

> $$
\left\{
\begin{array}{1}
\hat{\mu}=\bar{X_{n}}\\
\hat{\sigma^{2}}=\frac{1}{n}\sum\limits_{i=1}^{n}(X_{i}-\bar{X_{n}})^{2}\\
\end{array}
\right.$$  

> P.S.一定要注意，如果是对$\sigma^{2}$进行估计，那么求偏导的时候就要把整个当做一个整体。  
> 可以看到，用MLE来估计的方差的分母是n而不是样本方差中的(n-1)。所以显然它是一个有偏的估计量。

当无法使用求导来求极大值时，就应该使用其他方法来求。

> eg3:$f_{Y}(y)=e^{-(y-\theta)},y\geq\theta,\theta>0,Y^{n}=(Y_{1},Y_{2},...,Y_{n}).$  
> $L(\theta)=\prod\limits_{i=1}^{n}e^{-(y_{i}-\theta)}$  
> $\Rightarrow \rho(\theta)=ln(L(\theta))=\sum\limits_{i=1}^{n}\theta-y_{i}$  
> $\frac{d\rho(\theta)}{d\theta}=n>0$  
> 显然这里就没法说令其等于0，因为这个是一个递增函数没有极值点。但是因为它是递增的，如果我们在其定义域中取最大值就可以使函数最大化了。    
> $\because \theta\leq y,\quad \therefore \theta\leq min(y_{i})=y_{(1)}$  
> $\therefore \hat{\theta}=y_{(1)}$


**Moments Estimation**

首先我们要回忆一下关于矩的一些知识：

$$E(X^{k})=
\left\{
\begin{array}{11}
\sum_{x}x^{k}f(x) & discrete R.V.\\
\int x^{k}f(x)dx & continuous R.V.\\
\end{array}
\right.$$

矩估计的使用方法：

1. 首先，你所要估计的参数必须是各阶矩的函数，并且一般来说有s个未知函数，那么就应该用到s阶矩。
2. 我们所要做的仅仅是用样本矩去替代总体矩,作为总体矩的估计，然后求解出各个参数的估计表达式即可（解方程组）

*j阶样本矩*：
$$\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i}^{j}$$

> eg1:Suppose that $Y^{n}=(Y_{1},Y_{2},...,Y_{n})$ is a random sample of size n from a normal distribution with mean $\mu$ and variance $\sigma^{2}$.Estimate these two parameters.  
> $\mu=E(Y),\sigma^{2}=E(Y^{2})-(E(Y))^{2}$  
> $\Rightarrow \hat{\mu}=\hat{E(Y)}=\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i},\hat{\sigma^{2}}=\hat{E(Y^{2})}-(\hat{E(Y)})^{2}=\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i}^{2}-(\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i})^{2}$  
> 这就是矩估计量

> eg2:Suppose that $Y^{n}=(Y_{1},Y_{2},...,Y_{n})$ is a random sample of size n from a general gamma distribution with the pdf $f_{Y}(y,\alpha,\beta)=\frac{1}{\tau(\alpha)\beta^{\alpha}}y^{\alpha-1}e^{-\frac{y}{\beta}},for\,\,y>0$,where $\alpha,\beta>0$ and $\tau(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt$.Estimate $\alpha,\beta$  
> $E(Y)=\alpha\beta,Var(Y)=\alpha\beta^{2}$  
> $\Rightarrow \beta=\frac{E(Y^{2})-(E(Y))^{2}}{E(Y)},\alpha=\frac{(E(Y))^{2}}{E(Y^{2})-(E(Y))^{2}}$  
> $\therefore \hat{\beta}=\frac{\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i}^{2}-(\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i})^{2}}{\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i}},\hat{\alpha}=\frac{(\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i})^{2}}{\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i}^{2}-(\frac{1}{n}\sum\limits_{i=1}^{n}Y_{i})^{2}}$

*所以你会发现MME非常方便，因为它的过程非常简单不想MLE那样需要求极值之类的，同时MME没有直接用到分布的信息。但是我们可以看到，MME在某种程度上比较鸡肋，因为首先你需要估计的参数要能够被各阶矩表示出来，才能进行后面的替代，其次因为它的简便性，它的精确度就没有MLE那么高，毕竟它只使用了矩的信息，而没有使用整个分布的信息。*



##区间估计







