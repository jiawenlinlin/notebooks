---
title: "Theory of Probability"
author: "林嘉文"
date: "2017年8月19日"
output: html_document
---
#Chapter 1 Combinatorial Analysis

##Counting:Principle of Counting

说是叫counting，其实是一种连乘法则。我们在考虑“一件事”有多少种可能的时候，我们都会把这件事拆成“小事件”，这件事其实是由一步步的小事件组成的，所以我们顺次连乘某个大事件的所有小事件的可能事件数，就得到了这件事的可能事件数。比如说，如果有 *r* 个小事件，每个小事件有 $n_{i}$ 种可能结果，则总结果数可以表示成
$$n_{1}*n_{2}*n_{3}*...*n_{r}$$

*需要指出的是，我们在这里并不用纠结到底有没有独立的要求或者什么相关性，我们纯粹从计数的目的考虑，那么就算小事件之间有关联，我们认为计数总是没有错的。*


*所以我们现在的任务是计算出$n_{1}$、$n_{2}$、$n_{r}$的值。而计算它们的方法到最后无非两种，一种是排列，一种是组合。*


##Permutations:排列一般是针对n个个体一起进行

* 个体有区分：$n!$
* 个体无区分(与连乘法则不相符)：$\frac{n!}{n_{1}!*...}$ (分母部分除的就是认为无区分的个体)

##Combinations:组合一般是要求从n个个体中挑选一部分(例如r个个体)
* 挑出来的组的组内个体是有顺序的：$n*(n-1)*(n-2)...*(n-r+1)$ = $\frac{n!}{(n-r)!}$
* 挑出来的组的组内个体是无顺序的：$\frac{n*(n-1)*(n-2)...*(n-r+1)}{r!}$=$\frac{n!}{(n-r)!r!}=$=$C_{n}^{r}$


*key point:连乘法则其实在一次counting中被运用了很多次，因为对于一个大大事件，我们可以拆成多个大事件，每个大事件又可以拆成多个事件，每个事件又可以拆成多个小事件。我们可以把小事件理解为permutation和combination，它们也都在用连乘！但是，连乘法则的默认条件是**所有个体都不相同**，**所有抽出来的组内都有顺序**。所以每当我们使用permutation和combination时，如果出现个体相同的排列/组内无顺序的组合都先使用连乘法则，再将其转化为对应的情况，做法就是在分母做除法。*

> 我们可以简单举例，比如有A/B/C三个人，我们要从中抽取两个人，请问有多少种方法？这就是一个组内无顺序的组合。假如我们直接用连乘，那就是3*2=6种，但是其实AB和BA是一样的，AC和CA是一样的，BC和CB是一样的，即不考虑不同的顺序。所以实际上只有3种可能。

> n个人中有m个人是坏人，问有多少种线性排列使得坏人不会紧挨着？
> 逻辑就是：我们先排列(n-m)个好人，再把坏人插空放进去。
> $\frac{(n-m)!}{(n-m)!}\times\frac{(n-m+1)!}{(n-2m+1)!m!}$  
> 这个计算式的前半部分是一个排列，但是我们把所有好人当做相同的个体，就必须做除法，后半部分是一个组合，这就是一个无顺序的组合。

> 一个人有8个朋友，其中5个要被邀请去聚会。(a)如果8个人中2个人不可以同时去(b)如果8个人中2个人要么都去要么就都不去。请问有多少种选择  
> (a)逻辑就是没有限制的时候的选择数-2个人同时去的选择数。  
> $C_{8}^{5}-C_{6}^{3}$  
> (b)逻辑就是2个都去的选择数+2个都不去的选择数  
> $C_{6}^{3}+C_{6}^{5}$  
> 这是一个关于组合的题目，而且都是组内无顺序的。

##Binomial coefficient and multiple coefficients

* $C_{n}^{r}$ 就是二项式系数，它对应的就是把n个个体分成两组所对应的可能数，所以$C_{n}^{r}$=$C_{n}^{n-r}$，因为只要分出一组来，另一组也就确定了。
* 如果我们把n个个体分成r个不同的组，每组对应的大小为$n_{1}$、$n_{2}$、$n_{3}$、$\cdots$、$n_{r}$，并且$\sum n_{i}=n$，则对应的可能数就是$\frac{n!}{n_{1}!n_{2}!...n_{r}!}$。这就是多项式系数。其推导就是

##Binomial Theorem

$$(x+y)^{n}=\sum_{k=0}^{n}C_{n}^{k}x^{k}y^{n-k}$$
其证明可以使用induction method或者使用组合思想。

> 组合思想：这个$(x+y)^{n}$实际上就是n个(x+y)相乘，展开后可以想象到有很多项相加，但是每一项的次数总和都是n，那么在这个n次项中，可能是0个x，n个y构成的$y^{n}$，也可以是1个x，(n-1)个y构成的$xy^{n-1}$....而对于比如说$xy^{n-1}$，这个x可以从n个(x+y)中任意挑一个，所以用$C_{n}^{k}$表示挑出来的，且是没有顺序的。这样，我们就可以理解这个求和式为什么这么写了。

> proof:1.显然，当n=1时，等式成立。  
2.假设当n=t-1时成立，$(x+y)^{t-1}=\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-1-k}$  
3.当n=t时，$(x+y)^t$=$(x+y)^{t-1}(x+y)$=$(x+y)\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-1-k}$=$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k+1}y^{t-k-1}$+$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$\sum_{k=1}^{t}C_{t-1}^{k-1}x^{k}y^{t-k}$+$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$x^{t}$+$\sum_{k=1}^{t-1}C_{t-1}^{k-1}x^{k}y^{t-k}$+$y^t$+$\sum_{k=1}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$x^{t}$+$y^{t}$+$\sum_{k=1}^{t-1}C_{t}^{k}x^{k}y^{t-k}$=$\sum_{k=0}^{t}C_{t}^{k}x^{k}y^{t-k}$

##Multinomial Theorem

$$(x_{1}+x_{2}+...+x_{r})^{n}=\sum_{n_{1},n_{2},...,n_{r}}^{n_{1}+n_{2}+...+n_{r}=n}C_{n}^{n_{1},n_{2},...,n_{r}}(x_{1})^{n_{1}}(x_{2})^{n_{2}}\cdots(x_{r})^{n_{r}}$$

#Chapter 2 Axioms of Probability

##样本空间及事件

第一章所计算的结果是第二章的基础

*Definition:The **sample space S** of an experiment(whose outcome is uncertain) is the set of all possible outcomes of the experiment.*

*Any **subset E** of the sample space S is known as an **event**.If the outcome of the experiment is in E,then we say that E has occoured.*

> 一个样本空间有多少outcomes就需要用第一章的知识来计算。E就是S的一个子集，所以事件(event)就是一个集合，它是outcome的集合，某个事件可能只包含一个outcome，也可能包含多个outcome。比如，掷骰子，那么得到的结果{1,2,3,4,5,6}就是一个样本空间。事件“掷到6”对应的是一个outcome，但是事件“掷到奇数”对应的就是3个outcome。但是如果某个outcome进入到E这个集合中我们就说事件发生了，比如掷到3，那么我们就说掷到奇数这个事件发生了。

* 交集：Given events E and F ,$E\cap F$ is the set of all outcomes which are both in E and F.$E\cap F$ is also denoted $EF$.(intersection oF events)
* 并集：Given events E and F ,$E\cup F$ is the set of all outcomes either in E or F or both E and F.$E\cup F$ occours if either E or F occours.(union of events)
* 补集：For any event E ,$E^{c}$ denote the complement set of all outcomes in S which are not in E.We have $E\cup E^{c}=S$ and $E\cap E^{c}=\phi$
* 子集：For any two events E and F,we write $E\subset F$ as all the outcomes of E are in F.

$$E\cup F=F\cup E \qquad E\cap F=F\cap E$$
$$(E\cup F)\cup G=E\cup(F\cup G)$$
$$(E\cap F)\cap G=E\cap(F\cap G)$$
$$(E\cup F)\cap G=(E\cap G)\cup(F\cap G)$$
$$(E\cap F)\cup G=(E\cup G)\cap(F\cup G)$$

DeMorgan's law
$$(\cup_{i=1}^{n}E_{i})^{c}=\cap_{i=1}^{n}E_{i}^{c}$$
$$(\cap_{i=1}^{n}E_{i})^{c}=\cup_{i=1}^{n}E_{i}^{c}$$

> (i)对于$\forall x \in (\cup_{i=1}^{n}E_{i})^{c}$,则$x \notin \cup_{i=1}^{n}E_{i}$  
> $\Rightarrow x\notin E_{i},i=1,2,3,...,n$  
> $\Rightarrow x\in E_{1}^{c},x\in E_{2}^{c},\cdots$  
> $\Rightarrow x\in \cap_{i=1}^{n}E_{i}^{c}$  
> $\therefore (\cup_{i=1}^{n}E_{i})^{c}\subset\cap_{i=1}^{n}E_{i}^{c}$  
> 同理，对于$\forall x\in \cap_{i=1}^{n}E_{i}^{c}$,则$x\in E_{i}^{c},i=1,2,3,\cdots,n$  
> $\Rightarrow x\notin E_{i},i=1,2,3,\cdots,n$  
> $\Rightarrow x\notin \cup_{i=1}^{n}E_{i}$  
> $\Rightarrow x\in (\cup_{i=1}^{n}E_{i})^{c}$  
> $\therefore \cap_{i=1}^{n}E_{i}^{c}\subset(\cup_{i=1}^{n}E_{i})^{c}$  
> 因此得证  
> (ii)令$F_{i}=E_{i}^{c}$,则由(i)  
> $\Rightarrow \cap_{i=1}^{n}(F_{i}^{c})^{c}=(\cup_{i=1}^{n}F_{i}^{c})^{c}$  
> 对上式两边取补集，即得证

##概率

我们现在从事件进入到概率。

*Consider an experiment with sample space S.For each event E,we assume that a number P(E),the **probability** of the event E,is defined and satisfies the following 3 axioms.*

* Axiom 1: $0\leq P(E)\leq 1$
* Axiom 2: $P(S)=1$
* Axiom 3: For any sequence of **mutually exclusive** events $\{E_{i}\}_{i\geq 1}$,i.e.$E_{i}\cap E_{j}=\phi$ when $i\neq j$,then $$P(\cup_{i=1}^{\infty}E_{i})=\sum_{i=1}^{\infty}P(E_{i})$$

*Direct consequences include $P(\phi)=0$*

1. $P(E^{c})=1-P(E)

> $1=P(S)=P(E\cup E^{c})=P(E)+P(E^{c})$

2. If $E\subset F$ then $P(E)\leq P(F)$

> $P(F)=P((E\cup E^{c})\cap F)=P((E\cap F)\cup (E^{c}\cap F))=P(E\cap F)+P(E^{c}\cap F)=P(E)+P(E^{c}\cap F)$

3. $P(E\cup F)=P(E)+P(F)-P(E\cap F)$

> 容斥原理：$P(E_{1}\cup E_{2}\cdots\cup E_{n})=\sum_{i=1}^{n}P(E_{i})+\sum_{r=2}^{n-1}(-1)^{r+1}\sum_{i_{1}<i_{2}\cdots<i{r}}P(E_{i1}E_{i2}\cdots E_{ir})+(-1)^{n+1}P(E_{i1}\cdots E_{in})$

##等概率情况

*Assume S={1,2,...N} then it is often narural to assume P({i})=$\frac{1}{N}$.So for any event E,*

$$P(E)=\frac{\textrm{# of outcomes in E}}{\textrm{# of outcomes in S}}$$

#Chapter 3 Conditional probability and Independence

##条件概率及贝叶斯公式

*Conditional Probability:*$$P(E|F)=\frac{P(EF)}{P(F)}$$

*Equally likely outcomes:*$$P(E|F)=\frac{\textrm{# of outcomes in } E\cap F}{\textrm{# of outcomes in } F}=\frac{\frac{\textrm{# of outcomes in }E\cap F}{\textrm{# of outcomes in }S}}{\frac{\textrm{# of outcomes in }F}{\textrm{# of outcomes in }S}}$$

首先我们先理解一下条件概率的意思，就是要计算出当某件事发生之后，另一件事发生的概率。我们在很多时候就是要求这个，而不是单纯地求概率。其次我们来看一下计算的方式，其实分为两种。前者是一种reduced sample space的思想，也就是说在给定F发生时，样本空间已经发生了改变，所以分母是F的outcomes。后者就是纯粹利用计算公式。

*The mulTiplication Rule:Let $E_{1}$,$E_{2}$,$\cdots$,$E_{n}$ be a sequence of events,then we have,*
$$P(E_{1}\cap E_{2}\cap E_{3}...\cap E_{n})=P(E_{1})P(E_{2}|E_{1})P(E_{3}|E_{2}E_{1})...P(E_{n}|E_{1}E_{2}...E_{n-1})$$

*Bayes formula:*
$$P(E)=P(E\cap F)+P(E\cap F^{c})=P(E|F)P(F)+P(E|F^{c})P(F^{c})$$

> $\Rightarrow$ geceralized,assume  $\cup F_{i}=S$,$F_{i}\cap F_{j}=\phi$, then  
> $P(E)=P(E\cap (\cup F_{i}))=P(\cup(E\cap F_{i}))=\sum_{i=1}^{n}P(E\cap F_{i})=\sum_{i=1}^{n}P(E|F_{i})P(F_{i})$

贝叶斯公式的想法也非常非常自然，一件事情E发生的概率，其实可以转换成在另一组事件Fi的所有互斥可能发生的情况下这件事情发生的概率之和。举个例子就是——小明明天要去打球的概率，等同于明天晴天、阴天或者下雨天三种情况（假设明天天气只有这三种，它们也明显互斥）下小明去打球的概率之和。

##Independence and Conditional Independence

*We say that two events E,F are independent if*
$$P(E\cap F)=P(E)P(F)$$
*or equivalently*
$$P(E|F)=P(E) and P(F|E)=P(F)$$

> 如果E和F独立，则E和$F^{c}$也独立。（用贝叶斯公式很容易证明）

*We say that three events E,F,G are independent,if*
$$\nearrow P(E\cap F\cap G)=P(E)P(F)P(G)$$
$$\searrow P(E\cap F)=P(E)P(F),P(E\cap G)=P(E)P(G),P(F\cap G)=P(F)P(G)$$
*更一般的说，我们如果要说明n个事件独立，则要求这n个事件两两独立，三三独立，....，一直到n个事件独立。*

*We say that the events $T_{1},T_{2},T_{3}...T_{n}$ are conditionally independent given E(respectively $E^{c}$) if for any positive integer $2\leq r\leq n$,we have *
$$P(T_{i_{1}}\cap T_{i_{2}}\cap ...\cap T_{i_{r}}|E)=\prod_{i=1}^{r}P(T_{i_{j}}|E)$$

说白了就是在给定条件下满足事件独立。每个事件后面都加了一个条件。

#Chapter 4 Random Variables

##Definition of R.V.

*Real-valued functions defined on the sample space are random variables.*

> 所以随机变量就是一个函数，它的定义域就是sample space，它的值域就是函数的取值（实值）。更直白地说，因为定义域每一个outcome必然一一对应于某一个取值，所以我们把难以数理计算的sample space转为random variable处理。**这个函数的特殊性不在于我们找不到特定的函数形式，而在于它的定义域具有随机性这么一个最重要的特性。这就使得随机变量的取值具有随机性，而随机性我们用probability刻画**
> $$probability\longleftarrow outcomes\longrightarrow random\, variable$$
> 我们用outcome去推向probability这个概念，而random variable的取值就是outcome，所以random variable就有了一个概率分布，且所有取值的概率和为1$\Leftarrow P(S)=1$

P.S.如果我们从随机变量是为了把无法数理化的实验outcome数理化的角度来看，那么随机变量一一对应于outcome的取值其实无关紧要。例如掷色子，有1,2,3,4,5,6这些outcomes，你可以定义值域就是1,2,3,4,5,6也可以定义2,4,6,8,10,12，只不过我们一般定义都有实际意义。所以这里就可以看出，真正重要的东西是定义域的不确定性而不是映射本身到底是什么。

##Discrete R.V.

从这之后我们就只讨论随机变量，因为它具有可数理化的优点。并且，可以看到，随机变量这个东西其实就是从一些具体的实验中抽象出来的（比如掷色子、掷硬币）。而现在我们就彻底的抽象它，我们直接考虑随机变量的取值以及它的概率分布，但是对于它的定义域也就是sample space到底是什么实验并不关心，也不需要关心。

随机变量根据其值域可数还是不可数分为discrete R.V.和continuous R.V.我们现在先介绍离散随机变量

*pmf=probability mass function*
$$P(a)=P(X=a)$$
$$P(X_{i})\geq 0\,and\, \sum_{i=1}^{\infty}P(X_{i})=1$$

*CDF=culmulative distribution function*
$$F(a)=P(X\leq a)=\sum_{x:x\leq}P(x)$$

*Expected value*
$$E(x)=\sum_{i=1}^{\infty}x_{i}P(x_{i})\quad \textrm{实际上是一种加权，是对分布的一种概括}$$

* $E[g(x)]=\sum_{i}g(x_{i})P(x_{i})$  
首先，我们套一个g(x)的目的是因为我们认为不同的人对于不同事件的重视程度是不同的，但一系列事件的发生概率分布是固定的（因此我们用g(x)之后还可以用P(xi)就是由于事件的不确定性是核心，我们只是更换了函数的取值），那我们就可以通过改变值域来定制每一个人的utility

* $E(ax+b)=aE(x)+b$

*Variance*

*A random variable is entirely defined by its pmf but it's very useful to be able to summarize the essential properties of it through a few suitably defined measures*

均值是一种描述分布的方式，但它无法描述离散程度。两个离散程度差异很大的随机变量，它们的均值可以是一样的。因此我们给出了方差定义。

$$Var(X)=E[(X-\mu)^{2}]=E(X^{2})-[E(X)]^{2}$$
$$Std(X)=\sqrt{Var(X)}$$

* $Var(aX+b)=a^{2}Var(X)$

**Bernoulli Distribution**

它的sample space就是“实验成功”以及“实验失败”，X=1表示成功，X=0表示失败，从而构造R.V.
$$P(X=1)=p,P(X=0)=1-p$$
$$E(X)=p,Var(x)=p(1-p)$$

**Binomial Distribution**

Let X denote the # of success then we have $X=X_{1}+X_{2}+\cdots+X_{n}$ where $X_{i}$ is identical,independent distributed Bernoulli R.V. with probability p
$$P(X=k)=C_{n}^{k}p^{k}(1-p)^{n-k}$$
$$E(X)=np,Var(X)=np(1-p)$$

> 证明均值有两种思路。一种是直接用计算公式$E(X)=\sum xP(x)$,然后计算就好了。另一种是从X的构成来思考，因为$X_{i}$是i.i.d.的，因此求均值可以拆开。  
> 证明方差也是有两种思路，主要是针对$E(X^{2})$如何计算的问题。一种还是可以直接使用计算公式，$X^{2}$就相当于g(x)，另一种则使用构造新变量的方法。

**Poisson Distribution**

A discrete R.V. X taking values 0,1,2,... is said to be a Poisson R.V. with parameter $\lambda$,$\lambda>0$,if 
$$P(i)=P(X=i)=e^{-\lambda}\frac{\lambda^{i}}{i!}$$
$$\Rightarrow \sum_{i=1}^{\infty}P(i)=e^{-\lambda}\sum_{i=0}^{\infty}=e^{-\lambda}e^{\lambda}=1\quad\textrm{这是对于pmf的必然要求}$$

This expresses the probability of a number of events occuring in a fixed period of time if these events occour with a known average rate $\lambda$.**首先，很多现象的分布都可以用它进行解释，比如电话中心每分钟的电话数、伦敦每年的谋杀案数目.....但是要注意的是这个$\lambda$，必须根据你所设的X的不同而不同。他表示的是你的fixed time的发生平均强度**

eg：一小时有6辆公交车，如果你问5分钟内有可能来多少辆车，那么按照10分钟一辆车，则5分钟时间对应的$\lambda=0.5$，如果你问10分钟内有可能来多少辆车，那么对应的$\lambda=1$

$$E(X)=\sum_{i=0}^{\infty}i\cdot e^{-\lambda}\frac{\lambda^{i}}{i!}=\sum_{i=1}^{\infty}i\cdot e^{-\lambda}\frac{\lambda^{i}}{i!}=\sum_{i=1}^{\infty}e^{-\lambda}\frac{\lambda\cdot\lambda^{i-1}}{(i-1)!}=\lambda\cdot\sum_{j=0}^{\infty}e^{-\lambda}\frac{\lambda^{j}}{j!}=\lambda$$

$$Var(X)=E(X^{2})-[E(X)]^{2}=\sum_{i=0}^{\infty}i^{2}e^{-\lambda}\frac{\lambda^{i}}{i!}-\lambda^{2}=\lambda\cdot\sum_{j=0}^{\infty}(j+1)\cdot e^{-\lambda}\frac{\lambda^{j}}{j!}=\lambda\cdot E(X+1)-\lambda^{2}=\lambda$$

> *Poisson as an approximation to Binomial*  
> 当n非常大，p非常小使得np是一个较为适度的值时，Binomial(n,p)就可以很好地由poisson去近似。并且我们可以看到二项分布的均值为np，而二项分布的方差是np(1-p),都约等于$\lambda$

**Geometric Distribution**

考虑独立重复的Bernoulli实验，每一次成功概率为p。做实验直到有一次成功才停止。让X表示需要的实验次数。$X\in\{1,2,3,...\}$

$$P(X=n)=(1-p)^{n-1}\cdot p$$

> $\sum_{k=1}^{\infty}P(X=k)=\sum_{k=1}^{\infty}(1-p)^{k-1}\cdot p=p\cdot\sum_{k=1}^{\infty}(1-p)^{k-1}=1$ 符合pmf的要求

$$E(X)=\sum_{k=1}^{\infty}k\cdot(1-p)^{k-1}\cdot p=p\cdot\sum_{k=1}^{\infty}[-(1-p)^{k}]^{'}=-p\cdot[\sum_{k=1}^{\infty}(1-p)^{k}]^{'}=\frac{1}{p}$$
$$Var(X)=\frac{1-p}{p^{2}}$$

> eg:买方便面送水浒卡，总共有n种卡片。定义$X_{k}$为在获得(k-1)张不同卡片之后，要想得到第k个不一样的卡片需要买的方便面的袋数。我们假设去商店买到任何一张卡片的概率是$\frac{1}{n}$。问想要获得n种卡片，平均要买多少包方便面？  
> 解决逻辑：把问题细分到最底层，再反向一步步解决。而且题目已经提示得很明显了，$Y=X_{1}+X_{2}+X_{3}+...+X_{n}$,每一个X都是一个随机变量，我们就是要找$X_{k}$的分布。  
> $X_{k}$服从几何分布，重要的是找到对应的$p_{k}$。$p_{k}=\frac{抽到不同于之前(k-1)张卡的可能数}{总共可能抽到的可能个数}=\frac{n-k+1}{n}$
> $E(Y)=E(X_{1}+X_{2}+...+X_{n})=E(X_{1})+E(X_{2]})+...+E(X_{n})=\frac{1}{p_{1}}+\frac{1}{p_{2}}+...+\frac{1}{p_{n}}$

**Hypergeometric Distribution**

盒子里分别有黑球m个和白球(N-m)个，从N个球里不放回地抓n个，令X为黑球的个数。

$$P(X=i)=\frac{C_{m}^{i}C_{N-m}^{n-i}}{C_{N}^{n}}$$
$$E(X)=np$$
$$Var(X)=np(1-p)[1-\frac{n-1}{N-1}]$$

* 这种方法很适合于抽样中有两类，它们在整个N中分为m何N-m，你从中抽取n个，则你感兴趣的那一类抽出了多少个就满足这个分布。
* 当m比n大很多时，放回与不放回近似。同时，从均值和方差可以看出当N很大时，它和二项分布的均值和方差近似。

> eg1:Capture and Recapture Experiments
> 我们可以人工构造超几何分布。在一块区域标记m个动物然后放回去区域中，这样整个区域的N个动物中有m个是已标记的，(N-m)个是未标记的。这时我们再去抓n个，则抓到的标记动物个数X服从Hyper(N,m,n).  
> 这里的N为未知参数，而我们已知X的分布，显然可以用最大似然估计的方法来估计这个参数N  
> $\Rightarrow$ 找到N使得$\,P(X=x_{0})=\frac{C_{m}^{x_{0}}C_{N-m}^{n-x_{0}}}{C_{N}^{n}}\,$最大化      
> One can show that the maximum likelihood estimate is the largest integer value not exceeding $\frac{mn}{i}$

> eg:两个编排人员审阅小说《龙族》，一个审出a个错别字，另一个人审出b个错别字，两个人审出的相同错别字为c个。请估计总错别字数。  
> 解:总错别字数设为N，把第一个人审出的a个错别字当做标记，那么第二个人审出的b个错别字就相当于从这N个字里面抓b个，其中和第一个人相同的错别字数X服从Hyper(N,a,b)  
> $P(X=i)=\frac{C_{a}^{i}C_{N-a}^{b-i}}{C_{N}^{b}}$  
> 所以直接使用估计式得到$\Rightarrow\frac{ab}{c}$

#Chapter 5 Continuous R.V.

接着上一章的内容，R.V.的值域当然可以是*连续*，是*不可数尽*的，比如人的体重，等公交车的时间等等都是连续的值。

##Definition of continuous R.V.

*Formal definition:We say that X is a (real-valued) continuous R.V. if there exists a nonnegative function $f:\Re\rightarrow [0,\infty]$ such that for any set A of real number,*
$$P(X\in A)=\int_{A}f(x)dx$$

*f(x) is called the **probability density function(pdf)** of the R.V. X and associated **cumulative distribution function(cdf)** is*
$$F(x)=P(X\leq x)=\int_{-\infty}^{x}f(t)dt$$

*So we have*
$$f(x)=\frac{dF(x)}{dx}$$

> $\Rightarrow P(X\in\Re)=1=\int_{\Re}f(x)dx$ 这是pdf、pmf的必然要求

1. $\Re$是X的值域
2. 一定要记住X是一个特殊的函数，它的定义域是sample space所以具有随机性，而值域是$\Re$。而f(x)是X取某个值的概率密度，其实和离散时一样，它就好比某个outcome发生的概率。
3. 对于$f(\cdot)$而言，$\Re$是它的定义域，$[0,+\infty]$是它的值域。f(x)本身不是概率，但是它刻画了概率（通过积分来刻画）。

> 因为$P(X=x)=0$，所以$P(a\leq x\leq b)=P(a\leq x<b)=P(a<x\leq b)=P(a<x<b)$

##几个典型连续分布

**Uniform Distribution**

We have $c<d$ and $X\in[c,d]$

$$f(x)=\frac{1}{d-c},\textrm{when x}\in[c,d]$$

$$f(x)=0,otherwise$$

$$E(X)=\int_{-\infty}^{\infty}x\cdot f(x)dx=\frac{1}{d-c}\int_{c}^{d}xdx=\frac{c+d}{2},E(X^{2})=\frac{c^{2}+d^{2}+cd}{3}$$

$$Var(X)=E(x^{2})-[E(x)]^2=\frac{(d-c)^{2}}{12}$$

**Exponential Distribution**

We have $\lambda>0$ and $X\in[0,\infty)$

$$f(x)=\lambda\cdot e^{-\lambda x},\textrm{when x}\in[0,\infty)$$

$$f(x)=0,otherwise$$

$$E(X)=\int_{-\infty}^{\infty}x\cdot f(x)=\lambda\int_{0}^{\infty}x\cdot e^{-\lambda x}=\frac{1}{\lambda}$$

$$E(X^{2})=\int_{0}^{\infty}x^{2}\cdot f(x)=\lambda\int_{0}^{\infty}x^{2}e^{-\lambda x}=\frac{2}{\lambda^{2}}$$

$$Var(X)=E(X^{2})-[E(X)]^{2}=\frac{1}{\lambda^{2}}$$

*If the acerage rate of events is $\lambda$ in a given time interval,then it can be shown that the interval time between two events is a continuous R.V. with exponential distribution.*

**Normal Distribution/Gaussion Distribution**

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}},x\in(-\infty,\infty),\sigma\in[0,\infty)$$

> 写作$X\sim(\mu,\sigma^{2})$  
> $\int_{-\infty}^{\infty}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}dx=\sqrt{2\pi}\sigma$

正态分布本身是一个美丽的分布，它不仅是世界上绝大多数规律的展示，同时他还有很多性质。

1. Let X be a normal R.V. of parameters $(\mu,\sigma^{2})$ and consider the new R.V. Y such that $Y=aX+b$
$$\Rightarrow\, E(Y)=a\mu +b,Var(Y)=a^{2}\sigma^{2}$$
$$Y\sim N(a\mu+b,a^{2}\sigma^{2})$$

> proof.  
> $P(Y\leq y)=P(aX+b\leq y)=P(X\leq \frac{y-b}{a})=F_{X}(\frac{y-b}{a})$  
> $\therefore F_{Y}(y)=F_{X}(\frac{y-b}{a})$  
> $f_{Y}(y)=\frac{dF_{Y}(y)}{dy}=\frac{dF_{X}(\frac{y-b}{a})}{dy}=f_{X}(\frac{y-b}{a})\cdot\frac{1}{a}$  
> $\therefore f_{Y}(y)=\frac{1}{a}\cdot\frac{1}{\sqrt{2\pi\sigma}}\cdot e^{-\frac{(\frac{y-b}{a}-\mu)^{2}}{2\sigma^{2}}}=\frac{1}{\sqrt{2\pi}\sigma a}\cdot e^{-\frac{(y-b-\mu a)^{2}}{2\sigma^{2}a^{2}}}$ 

2. Standardizing  
Let X a normal R.V. of mean $\mu$ and variance $\sigma^{2}$
$$Z=\frac{X-\mu}{\sigma}\sim N(0,1)是standard\,normal\,R.V. $$
$$P(a\leq X\leq b)=P(\frac{a-\mu}{\sigma}\leq\frac{X-\mu}{\sigma}\leq\frac{b-\mu}{\sigma})=\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma})$$

3. CDF of $X\sim N(0,1)$
$$\Phi(x)=P(X\leq x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-\frac{t^{2}}{2}}dt$$

> 对于$\Phi(x)$,有$\Phi(-x)=1-\Phi(x)$

4. 3$\sigma$原则
$$P(\mu-\sigma\leq X\leq \mu+\sigma)\approx0.68$$
$$P(\mu-2\sigma\leq X\leq \mu+2\sigma)\approx0.95$$
$$P(\mu-3\sigma\leq X\leq \mu+3\sigma)\approx0.997$$

这里的X是服从均值为$\mu$方差为$\sigma^{2}$的正态分布，因此通过标准化之后就可以查表得到结果。这个原则告诉我们一个正态分布的随机变量其取值在3个标准差之内的概率几乎就是1。因此如果出现了超出3$\sigma$的值，那么我们认为有问题。

5. Normal approximation to the Binomial distribution  
之前我们说过用泊松去近似二项分布，现在可以用正态分布去近似二项分布

$$As\,n\rightarrow\infty,with\,\mu=np\,\,and\,\, \sigma^{2}=np(1-p),X\sim Bin(n,p)$$
$$P(a\leq X\leq b)=P(\frac{a-np}{\sqrt{np(1-p)}}\leq\frac{X-np}{\sqrt{np(1-p)}}\leq\frac{b-np}{\sqrt{np(1-p)}})\approx\Phi()-\Phi()$$

用正态的话并不要求p很小，只要均值和方差是有限值即可。因此相对来说会比用泊松来近似的条件要求的少。

> * $E(x)=arg_{a}\,min\,E(X-a)^{2}$  
> * $m(x)=arg_{a}\,min\,E|X-a|$   
> The median of a continuous R.V. X of pdf f(x) is the number such that $\int_{-\infty}^{m}f(x)dx=\int_{m}^{\infty}f(x)dx=\frac{1}{2}\Leftrightarrow P(X\leq m)=P(X\geq m)=\frac{1}{2}$ 


##Other continuous distribution

**Log-normal Distribution**

X follow a log-normal $(\mu,\sigma)$ Distribution if its pdf
$$f_{X}(x)=\frac{1}{\sqrt{2\pi}\sigma}\frac{1}{x}e^{-\frac{(logX-\mu)^{2}}{2\sigma^{2}}},\quad x\in(0,\infty)$$
$$f_{X}(x)=0,\quad otherwise$$

> If X follows a log-normal$(\mu,\sigma)$,then $logX\sim N(\mu,\sigma^{2})$

**Gamma Distribution**

**Beta Distribution**

##Change of variables

一般来说就是，给出X的pdf，然后要你求$Y=g(X)$的pdf

> 永远从CDF入手，先找$F_{Y}(y)=P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))$  
> 再运用$f(y)=\frac{dF_{Y}(y)}{dy}$求解