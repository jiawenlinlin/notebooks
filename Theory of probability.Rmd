---
title: "Theory of Probability"
author: "林嘉文"
date: "2017年8月19日"
output: html_document
---
#Chapter 1 Combinatorial Analysis

##Counting:Principle of Counting

说是叫counting，其实是一种连乘法则。我们在考虑“一件事”有多少种可能的时候，我们都会把这件事拆成“小事件”，这件事其实是由一步步的小事件组成的，所以我们顺次连乘某个大事件的所有小事件的可能事件数，就得到了这件事的可能事件数。比如说，如果有 *r* 个小事件，每个小事件有 $n_{i}$ 种可能结果，则总结果数可以表示成
$$n_{1}*n_{2}*n_{3}*...*n_{r}$$

*需要指出的是，我们在这里并不用纠结到底有没有独立的要求或者什么相关性，我们纯粹从计数的目的考虑，那么就算小事件之间有关联，我们认为计数总是没有错的。*


*所以我们现在的任务是计算出$n_{1}$、$n_{2}$、$n_{r}$的值。而计算它们的方法到最后无非两种，一种是排列，一种是组合。*


##Permutations:排列一般是针对n个个体一起进行

* 个体有区分：$n!$
* 个体无区分(与连乘法则不相符)：$\frac{n!}{n_{1}!*...}$ (分母部分除的就是认为无区分的个体)

##Combinations:组合一般是要求从n个个体中挑选一部分(例如r个个体)
* 挑出来的组的组内个体是有顺序的：$n*(n-1)*(n-2)...*(n-r+1)$ = $\frac{n!}{(n-r)!}$
* 挑出来的组的组内个体是无顺序的：$\frac{n*(n-1)*(n-2)...*(n-r+1)}{r!}$=$\frac{n!}{(n-r)!r!}=$=$C_{n}^{r}$


*key point:连乘法则其实在一次counting中被运用了很多次，因为对于一个大大事件，我们可以拆成多个大事件，每个大事件又可以拆成多个事件，每个事件又可以拆成多个小事件。我们可以把小事件理解为permutation和combination，它们也都在用连乘！但是，连乘法则的默认条件是**所有个体都不相同**，**所有抽出来的组内都有顺序**。所以每当我们使用permutation和combination时，如果出现个体相同的排列/组内无顺序的组合都先使用连乘法则，再将其转化为对应的情况，做法就是在分母做除法。*

> 我们可以简单举例，比如有A/B/C三个人，我们要从中抽取两个人，请问有多少种方法？这就是一个组内无顺序的组合。假如我们直接用连乘，那就是3*2=6种，但是其实AB和BA是一样的，AC和CA是一样的，BC和CB是一样的，即不考虑不同的顺序。所以实际上只有3种可能。

> n个人中有m个人是坏人，问有多少种线性排列使得坏人不会紧挨着？
> 逻辑就是：我们先排列(n-m)个好人，再把坏人插空放进去。
> $\frac{(n-m)!}{(n-m)!}\times\frac{(n-m+1)!}{(n-2m+1)!m!}$  
> 这个计算式的前半部分是一个排列，但是我们把所有好人当做相同的个体，就必须做除法，后半部分是一个组合，这就是一个无顺序的组合。

> 一个人有8个朋友，其中5个要被邀请去聚会。(a)如果8个人中2个人不可以同时去(b)如果8个人中2个人要么都去要么就都不去。请问有多少种选择  
> (a)逻辑就是没有限制的时候的选择数-2个人同时去的选择数。  
> $C_{8}^{5}-C_{6}^{3}$  
> (b)逻辑就是2个都去的选择数+2个都不去的选择数  
> $C_{6}^{3}+C_{6}^{5}$  
> 这是一个关于组合的题目，而且都是组内无顺序的。

##Binomial coefficient and multiple coefficients

* $C_{n}^{r}$ 就是二项式系数，它对应的就是把n个个体分成两组所对应的可能数，所以$C_{n}^{r}$=$C_{n}^{n-r}$，因为只要分出一组来，另一组也就确定了。
* 如果我们把n个个体分成r个不同的组，每组对应的大小为$n_{1}$、$n_{2}$、$n_{3}$、$\cdots$、$n_{r}$，并且$\sum n_{i}=n$，则对应的可能数就是$\frac{n!}{n_{1}!n_{2}!...n_{r}!}$。这就是多项式系数。其推导就是

##Binomial Theorem

$$(x+y)^{n}=\sum_{k=0}^{n}C_{n}^{k}x^{k}y^{n-k}$$
其证明可以使用induction method或者使用组合思想。

> 组合思想：这个$(x+y)^{n}$实际上就是n个(x+y)相乘，展开后可以想象到有很多项相加，但是每一项的次数总和都是n，那么在这个n次项中，可能是0个x，n个y构成的$y^{n}$，也可以是1个x，(n-1)个y构成的$xy^{n-1}$....而对于比如说$xy^{n-1}$，这个x可以从n个(x+y)中任意挑一个，所以用$C_{n}^{k}$表示挑出来的，且是没有顺序的。这样，我们就可以理解这个求和式为什么这么写了。

> proof:1.显然，当n=1时，等式成立。  
2.假设当n=t-1时成立，$(x+y)^{t-1}=\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-1-k}$  
3.当n=t时，$(x+y)^t$=$(x+y)^{t-1}(x+y)$=$(x+y)\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-1-k}$=$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k+1}y^{t-k-1}$+$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$\sum_{k=1}^{t}C_{t-1}^{k-1}x^{k}y^{t-k}$+$\sum_{k=0}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$x^{t}$+$\sum_{k=1}^{t-1}C_{t-1}^{k-1}x^{k}y^{t-k}$+$y^t$+$\sum_{k=1}^{t-1}C_{t-1}^{k}x^{k}y^{t-k}$=$x^{t}$+$y^{t}$+$\sum_{k=1}^{t-1}C_{t}^{k}x^{k}y^{t-k}$=$\sum_{k=0}^{t}C_{t}^{k}x^{k}y^{t-k}$

##Multinomial Theorem

$$(x_{1}+x_{2}+...+x_{r})^{n}=\sum_{n_{1},n_{2},...,n_{r}}^{n_{1}+n_{2}+...+n_{r}=n}C_{n}^{n_{1},n_{2},...,n_{r}}(x_{1})^{n_{1}}(x_{2})^{n_{2}}\cdots(x_{r})^{n_{r}}$$

#Chapter 2 Axioms of Probability

##样本空间及事件

第一章所计算的结果是第二章的基础

*Definition:The **sample space S** of an experiment(whose outcome is uncertain) is the set of all possible outcomes of the experiment.*

*Any **subset E** of the sample space S is known as an **event**.If the outcome of the experiment is in E,then we say that E has occoured.*

> 一个样本空间有多少outcomes就需要用第一章的知识来计算。E就是S的一个子集，所以事件(event)就是一个集合，它是outcome的集合，某个事件可能只包含一个outcome，也可能包含多个outcome。比如，掷骰子，那么得到的结果{1,2,3,4,5,6}就是一个样本空间。事件“掷到6”对应的是一个outcome，但是事件“掷到奇数”对应的就是3个outcome。但是如果某个outcome进入到E这个集合中我们就说事件发生了，比如掷到3，那么我们就说掷到奇数这个事件发生了。

* 交集：Given events E and F ,$E\cap F$ is the set of all outcomes which are both in E and F.$E\cap F$ is also denoted $EF$.(intersection oF events)
* 并集：Given events E and F ,$E\cup F$ is the set of all outcomes either in E or F or both E and F.$E\cup F$ occours if either E or F occours.(union of events)
* 补集：For any event E ,$E^{c}$ denote the complement set of all outcomes in S which are not in E.We have $E\cup E^{c}=S$ and $E\cap E^{c}=\phi$
* 子集：For any two events E and F,we write $E\subset F$ as all the outcomes of E are in F.

$$E\cup F=F\cup E \qquad E\cap F=F\cap E$$
$$(E\cup F)\cup G=E\cup(F\cup G)$$
$$(E\cap F)\cap G=E\cap(F\cap G)$$
$$(E\cup F)\cap G=(E\cap G)\cup(F\cap G)$$
$$(E\cap F)\cup G=(E\cup G)\cap(F\cup G)$$

DeMorgan's law
$$(\cup_{i=1}^{n}E_{i})^{c}=\cap_{i=1}^{n}E_{i}^{c}$$
$$(\cap_{i=1}^{n}E_{i})^{c}=\cup_{i=1}^{n}E_{i}^{c}$$

> (i)对于$\forall x \in (\cup_{i=1}^{n}E_{i})^{c}$,则$x \notin \cup_{i=1}^{n}E_{i}$  
> $\Rightarrow x\notin E_{i},i=1,2,3,...,n$  
> $\Rightarrow x\in E_{1}^{c},x\in E_{2}^{c},\cdots$  
> $\Rightarrow x\in \cap_{i=1}^{n}E_{i}^{c}$  
> $\therefore (\cup_{i=1}^{n}E_{i})^{c}\subset\cap_{i=1}^{n}E_{i}^{c}$  
> 同理，对于$\forall x\in \cap_{i=1}^{n}E_{i}^{c}$,则$x\in E_{i}^{c},i=1,2,3,\cdots,n$  
> $\Rightarrow x\notin E_{i},i=1,2,3,\cdots,n$  
> $\Rightarrow x\notin \cup_{i=1}^{n}E_{i}$  
> $\Rightarrow x\in (\cup_{i=1}^{n}E_{i})^{c}$  
> $\therefore \cap_{i=1}^{n}E_{i}^{c}\subset(\cup_{i=1}^{n}E_{i})^{c}$  
> 因此得证  
> (ii)令$F_{i}=E_{i}^{c}$,则由(i)  
> $\Rightarrow \cap_{i=1}^{n}(F_{i}^{c})^{c}=(\cup_{i=1}^{n}F_{i}^{c})^{c}$  
> 对上式两边取补集，即得证

##概率

我们现在从事件进入到概率。

*Consider an experiment with sample space S.For each event E,we assume that a number P(E),the **probability** of the event E,is defined and satisfies the following 3 axioms.*

* Axiom 1: $0\leq P(E)\leq 1$
* Axiom 2: $P(S)=1$
* Axiom 3: For any sequence of **mutually exclusive** events $\{E_{i}\}_{i\geq 1}$,i.e.$E_{i}\cap E_{j}=\phi$ when $i\neq j$,then $$P(\cup_{i=1}^{\infty}E_{i})=\sum_{i=1}^{\infty}P(E_{i})$$

*Direct consequences include $P(\phi)=0$*

1. $P(E^{c})=1-P(E)

> $1=P(S)=P(E\cup E^{c})=P(E)+P(E^{c})$

2. If $E\subset F$ then $P(E)\leq P(F)$

> $P(F)=P((E\cup E^{c})\cap F)=P((E\cap F)\cup (E^{c}\cap F))=P(E\cap F)+P(E^{c}\cap F)=P(E)+P(E^{c}\cap F)$

3. $P(E\cup F)=P(E)+P(F)-P(E\cap F)$

> 容斥原理：$P(E_{1}\cup E_{2}\cdots\cup E_{n})=\sum_{i=1}^{n}P(E_{i})+\sum_{r=2}^{n-1}(-1)^{r+1}\sum_{i_{1}<i_{2}\cdots<i{r}}P(E_{i1}E_{i2}\cdots E_{ir})+(-1)^{n+1}P(E_{i1}\cdots E_{in})$

##等概率情况

*Assume S={1,2,...N} then it is often narural to assume P({i})=$\frac{1}{N}$.So for any event E,*

$$P(E)=\frac{\textrm{# of outcomes in E}}{\textrm{# of outcomes in S}}$$

#Chapter 3 Conditional probability and Independence

##条件概率及贝叶斯公式

*Conditional Probability:*$$P(E|F)=\frac{P(EF)}{P(F)}$$

*Equally likely outcomes:*$$P(E|F)=\frac{\textrm{# of outcomes in } E\cap F}{\textrm{# of outcomes in } F}=\frac{\frac{\textrm{# of outcomes in }E\cap F}{\textrm{# of outcomes in }S}}{\frac{\textrm{# of outcomes in }F}{\textrm{# of outcomes in }S}}$$

首先我们先理解一下条件概率的意思，就是要计算出当某件事发生之后，另一件事发生的概率。我们在很多时候就是要求这个，而不是单纯地求概率。其次我们来看一下计算的方式，其实分为两种。前者是一种reduced sample space的思想，也就是说在给定F发生时，样本空间已经发生了改变，所以分母是F的outcomes。后者就是纯粹利用计算公式。

*The mulTiplication Rule:Let $E_{1}$,$E_{2}$,$\cdots$,$E_{n}$ be a sequence of events,then we have,*
$$P(E_{1}\cap E_{2}\cap E_{3}...\cap E_{n})=P(E_{1})P(E_{2}|E_{1})P(E_{3}|E_{2}E_{1})...P(E_{n}|E_{1}E_{2}...E_{n-1})$$

*Bayes formula:*
$$P(E)=P(E\cap F)+P(E\cap F^{c})=P(E|F)P(F)+P(E|F^{c})P(F^{c})$$

> $\Rightarrow$ geceralized,assume  $\cup F_{i}=S$,$F_{i}\cap F_{j}=\phi$, then  
> $P(E)=P(E\cap (\cup F_{i}))=P(\cup(E\cap F_{i}))=\sum_{i=1}^{n}P(E\cap F_{i})=\sum_{i=1}^{n}P(E|F_{i})P(F_{i})$

贝叶斯公式的想法也非常非常自然，一件事情E发生的概率，其实可以转换成在另一组事件Fi的所有互斥可能发生的情况下这件事情发生的概率之和。举个例子就是——小明明天要去打球的概率，等同于明天晴天、阴天或者下雨天三种情况（假设明天天气只有这三种，它们也明显互斥）下小明去打球的概率之和。

##Independence and Conditional Independence

*We say that two events E,F are independent if*
$$P(E\cap F)=P(E)P(F)$$
*or equivalently*
$$P(E|F)=P(E) and P(F|E)=P(F)$$

> 如果E和F独立，则E和$F^{c}$也独立。（用贝叶斯公式很容易证明）

*We say that three events E,F,G are independent,if*
$$\nearrow P(E\cap F\cap G)=P(E)P(F)P(G)$$
$$\searrow P(E\cap F)=P(E)P(F),P(E\cap G)=P(E)P(G),P(F\cap G)=P(F)P(G)$$
*更一般的说，我们如果要说明n个事件独立，则要求这n个事件两两独立，三三独立，....，一直到n个事件独立。*

*We say that the events $T_{1},T_{2},T_{3}...T_{n}$ are conditionally independent given E(respectively $E^{c}$) if for any positive integer $2\leq r\leq n$,we have *
$$P(T_{i_{1}}\cap T_{i_{2}}\cap ...\cap T_{i_{r}}|E)=\prod_{i=1}^{r}P(T_{i_{j}}|E)$$

说白了就是在给定条件下满足事件独立。每个事件后面都加了一个条件。

#Chapter 4 Random Variables

##Definition of R.V.

*Real-valued functions defined on the sample space are random variables.*

> 所以随机变量就是一个函数，它的定义域就是sample space，它的值域就是函数的取值（实值）。更直白地说，因为定义域每一个outcome必然一一对应于某一个取值，所以我们把难以数理计算的sample space转为random variable处理。**这个函数的特殊性不在于我们找不到特定的函数形式，而在于它的定义域具有随机性这么一个最重要的特性。这就使得随机变量的取值具有随机性，而随机性我们用probability刻画**
> $$probability\longleftarrow outcomes\longrightarrow random\, variable$$
> 我们用outcome去推向probability这个概念，而random variable的取值就是outcome，所以random variable就有了一个概率分布，且所有取值的概率和为1$\Leftarrow P(S)=1$

P.S.如果我们从随机变量是为了把无法数理化的实验outcome数理化的角度来看，那么随机变量一一对应于outcome的取值其实无关紧要。例如掷色子，有1,2,3,4,5,6这些outcomes，你可以定义值域就是1,2,3,4,5,6也可以定义2,4,6,8,10,12，只不过我们一般定义都有实际意义。所以这里就可以看出，真正重要的东西是定义域的不确定性而不是映射本身到底是什么。

##Discrete R.V.

从这之后我们就只讨论随机变量，因为它具有可数理化的优点。并且，可以看到，随机变量这个东西其实就是从一些具体的实验中抽象出来的（比如掷色子、掷硬币）。而现在我们就彻底的抽象它，我们直接考虑随机变量的取值以及它的概率分布，但是对于它的定义域也就是sample space到底是什么实验并不关心，也不需要关心。

随机变量根据其值域可数还是不可数分为discrete R.V.和continuous R.V.我们现在先介绍离散随机变量

*pmf=probability mass function*
$$P(a)=P(X=a)$$
$$P(X_{i})\geq 0\,and\, \sum_{i=1}^{\infty}P(X_{i})=1$$

*CDF=culmulative distribution function*
$$F(a)=P(X\leq a)=\sum_{x:x\leq}P(x)$$

*Expected value*
$$E(x)=\sum_{i=1}^{\infty}x_{i}P(x_{i})\quad \textrm{实际上是一种加权，是对分布的一种概括}$$

* $E[g(x)]=\sum_{i}g(x_{i})P(x_{i})$  
首先，我们套一个g(x)的目的是因为我们认为不同的人对于不同事件的重视程度是不同的，但一系列事件的发生概率分布是固定的（因此我们用g(x)之后还可以用P(xi)就是由于事件的不确定性是核心，我们只是更换了函数的取值），那我们就可以通过改变值域来定制每一个人的utility

* $E(ax+b)=aE(x)+b$

*Variance*

*A random variable is entirely defined by its pmf but it's very useful to be able to summarize the essential properties of it through a few suitably defined measures*

均值是一种描述分布的方式，但它无法描述离散程度。两个离散程度差异很大的随机变量，它们的均值可以是一样的。因此我们给出了方差定义。

$$Var(X)=E[(X-\mu)^{2}]=E(X^{2})-[E(X)]^{2}$$
$$Std(X)=\sqrt{Var(X)}$$

* $Var(aX+b)=a^{2}Var(X)$

**Bernoulli Distribution**

它的sample space就是“实验成功”以及“实验失败”，X=1表示成功，X=0表示失败，从而构造R.V.
$$P(X=1)=p,P(X=0)=1-p$$
$$E(X)=p,Var(x)=p(1-p)$$

**Binomial Distribution**

Let X denote the # of success then we have $X=X_{1}+X_{2}+\cdots+X_{n}$ where $X_{i}$ is identical,independent distributed Bernoulli R.V. with probability p
$$P(X=k)=C_{n}^{k}p^{k}(1-p)^{n-k}$$
$$E(X)=np,Var(X)=np(1-p)$$

> 证明均值有两种思路。一种是直接用计算公式$E(X)=\sum xP(x)$,然后计算就好了。另一种是从X的构成来思考，因为$X_{i}$是i.i.d.的，因此求均值可以拆开。  
> 证明方差也是有两种思路，主要是针对$E(X^{2})$如何计算的问题。一种还是可以直接使用计算公式，$X^{2}$就相当于g(x)，另一种则使用构造新变量的方法。

**Poisson Distribution**

A discrete R.V. X taking values 0,1,2,... is said to be a Poisson R.V. with parameter $\lambda$,$\lambda>0$,if 
$$P(i)=P(X=i)=e^{-\lambda}\frac{\lambda^{i}}{i!}$$
$$\Rightarrow \sum_{i=1}^{\infty}P(i)=e^{-\lambda}\sum_{i=0}^{\infty}=e^{-\lambda}e^{\lambda}=1\quad\textrm{这是对于pmf的必然要求}$$

This expresses the probability of a number of events occuring in a fixed period of time if these events occour with a known average rate $\lambda$.**首先，很多现象的分布都可以用它进行解释，比如电话中心每分钟的电话数、伦敦每年的谋杀案数目.....但是要注意的是这个$\lambda$，必须根据你所设的X的不同而不同。他表示的是你的fixed time的发生平均强度**

eg：一小时有6辆公交车，如果你问5分钟内有可能来多少辆车，那么按照10分钟一辆车，则5分钟时间对应的$\lambda=0.5$，如果你问10分钟内有可能来多少辆车，那么对应的$\lambda=1$

$$E(X)=\sum_{i=0}^{\infty}i\cdot e^{-\lambda}\frac{\lambda^{i}}{i!}=\sum_{i=1}^{\infty}i\cdot e^{-\lambda}\frac{\lambda^{i}}{i!}=\sum_{i=1}^{\infty}e^{-\lambda}\frac{\lambda\cdot\lambda^{i-1}}{(i-1)!}=\lambda\cdot\sum_{j=0}^{\infty}e^{-\lambda}\frac{\lambda^{j}}{j!}=\lambda$$

$$Var(X)=E(X^{2})-[E(X)]^{2}=\sum_{i=0}^{\infty}i^{2}e^{-\lambda}\frac{\lambda^{i}}{i!}-\lambda^{2}=\lambda\cdot\sum_{j=0}^{\infty}(j+1)\cdot e^{-\lambda}\frac{\lambda^{j}}{j!}=\lambda\cdot E(X+1)-\lambda^{2}=\lambda$$

> *Poisson as an approximation to Binomial*  
> 当n非常大，p非常小使得np是一个较为适度的值时，Binomial(n,p)就可以很好地由poisson去近似。并且我们可以看到二项分布的均值为np，而二项分布的方差是np(1-p),都约等于$\lambda$

**Geometric Distribution**

考虑独立重复的Bernoulli实验，每一次成功概率为p。做实验直到有一次成功才停止。让X表示需要的实验次数。$X\in\{1,2,3,...\}$

$$P(X=n)=(1-p)^{n-1}\cdot p$$

> $\sum_{k=1}^{\infty}P(X=k)=\sum_{k=1}^{\infty}(1-p)^{k-1}\cdot p=p\cdot\sum_{k=1}^{\infty}(1-p)^{k-1}=1$ 符合pmf的要求

$$E(X)=\sum_{k=1}^{\infty}k\cdot(1-p)^{k-1}\cdot p=p\cdot\sum_{k=1}^{\infty}[-(1-p)^{k}]^{'}=-p\cdot[\sum_{k=1}^{\infty}(1-p)^{k}]^{'}=\frac{1}{p}$$
$$Var(X)=\frac{1-p}{p^{2}}$$

> eg:买方便面送水浒卡，总共有n种卡片。定义$X_{k}$为在获得(k-1)张不同卡片之后，要想得到第k个不一样的卡片需要买的方便面的袋数。我们假设去商店买到任何一张卡片的概率是$\frac{1}{n}$。问想要获得n种卡片，平均要买多少包方便面？  
> 解决逻辑：把问题细分到最底层，再反向一步步解决。而且题目已经提示得很明显了，$Y=X_{1}+X_{2}+X_{3}+...+X_{n}$,每一个X都是一个随机变量，我们就是要找$X_{k}$的分布。  
> $X_{k}$服从几何分布，重要的是找到对应的$p_{k}$。$p_{k}=\frac{抽到不同于之前(k-1)张卡的可能数}{总共可能抽到的可能个数}=\frac{n-k+1}{n}$
> $E(Y)=E(X_{1}+X_{2}+...+X_{n})=E(X_{1})+E(X_{2]})+...+E(X_{n})=\frac{1}{p_{1}}+\frac{1}{p_{2}}+...+\frac{1}{p_{n}}$

**Hypergeometric Distribution**

盒子里分别有黑球m个和白球(N-m)个，从N个球里不放回地抓n个，令X为黑球的个数。

$$P(X=i)=\frac{C_{m}^{i}C_{N-m}^{n-i}}{C_{N}^{n}}$$
$$E(X)=np$$
$$Var(X)=np(1-p)[1-\frac{n-1}{N-1}]$$

* 这种方法很适合于抽样中有两类，它们在整个N中分为m何N-m，你从中抽取n个，则你感兴趣的那一类抽出了多少个就满足这个分布。
* 当m比n大很多时，放回与不放回近似。同时，从均值和方差可以看出当N很大时，它和二项分布的均值和方差近似。

> eg1:Capture and Recapture Experiments
> 我们可以人工构造超几何分布。在一块区域标记m个动物然后放回去区域中，这样整个区域的N个动物中有m个是已标记的，(N-m)个是未标记的。这时我们再去抓n个，则抓到的标记动物个数X服从Hyper(N,m,n).  
> 这里的N为未知参数，而我们已知X的分布，显然可以用最大似然估计的方法来估计这个参数N  
> $\Rightarrow$ 找到N使得$\,P(X=x_{0})=\frac{C_{m}^{x_{0}}C_{N-m}^{n-x_{0}}}{C_{N}^{n}}\,$最大化      
> One can show that the maximum likelihood estimate is the largest integer value not exceeding $\frac{mn}{i}$

> eg:两个编排人员审阅小说《龙族》，一个审出a个错别字，另一个人审出b个错别字，两个人审出的相同错别字为c个。请估计总错别字数。  
> 解:总错别字数设为N，把第一个人审出的a个错别字当做标记，那么第二个人审出的b个错别字就相当于从这N个字里面抓b个，其中和第一个人相同的错别字数X服从Hyper(N,a,b)  
> $P(X=i)=\frac{C_{a}^{i}C_{N-a}^{b-i}}{C_{N}^{b}}$  
> 所以直接使用估计式得到$\Rightarrow\frac{ab}{c}$

#Chapter 5 Continuous R.V.

接着上一章的内容，R.V.的值域当然可以是*连续*，是*不可数尽*的，比如人的体重，等公交车的时间等等都是连续的值。

##Definition of continuous R.V.

*Formal definition:We say that X is a (real-valued) continuous R.V. if there exists a nonnegative function $f:\Re\rightarrow [0,\infty]$ such that for any set A of real number,*
$$P(X\in A)=\int_{A}f(x)dx$$

*f(x) is called the **probability density function(pdf)** of the R.V. X and associated **cumulative distribution function(cdf)** is*
$$F(x)=P(X\leq x)=\int_{-\infty}^{x}f(t)dt$$

*So we have*
$$f(x)=\frac{dF(x)}{dx}$$

> $\Rightarrow P(X\in\Re)=1=\int_{\Re}f(x)dx$ 这是pdf、pmf的必然要求

1. $\Re$是X的值域
2. 一定要记住X是一个特殊的函数，它的定义域是sample space所以具有随机性，而值域是$\Re$。而f(x)是X取某个值的概率密度，其实和离散时一样，它就好比某个outcome发生的概率。
3. 对于$f(\cdot)$而言，$\Re$是它的定义域，$[0,+\infty]$是它的值域。f(x)本身不是概率，但是它刻画了概率（通过积分来刻画）。

> 因为$P(X=x)=0$，所以$P(a\leq x\leq b)=P(a\leq x<b)=P(a<x\leq b)=P(a<x<b)$

##几个典型连续分布

**Uniform Distribution**

We have $c<d$ and $X\in[c,d]$

$$f(x)=\frac{1}{d-c},\textrm{when x}\in[c,d]$$

$$f(x)=0,otherwise$$

$$E(X)=\int_{-\infty}^{\infty}x\cdot f(x)dx=\frac{1}{d-c}\int_{c}^{d}xdx=\frac{c+d}{2},E(X^{2})=\frac{c^{2}+d^{2}+cd}{3}$$

$$Var(X)=E(x^{2})-[E(x)]^2=\frac{(d-c)^{2}}{12}$$

**Exponential Distribution**

We have $\lambda>0$ and $X\in[0,\infty)$

$$f(x)=\lambda\cdot e^{-\lambda x},\textrm{when x}\in[0,\infty)$$

$$f(x)=0,otherwise$$

$$E(X)=\int_{-\infty}^{\infty}x\cdot f(x)=\lambda\int_{0}^{\infty}x\cdot e^{-\lambda x}=\frac{1}{\lambda}$$

$$E(X^{2})=\int_{0}^{\infty}x^{2}\cdot f(x)=\lambda\int_{0}^{\infty}x^{2}e^{-\lambda x}=\frac{2}{\lambda^{2}}$$

$$Var(X)=E(X^{2})-[E(X)]^{2}=\frac{1}{\lambda^{2}}$$

*If the acerage rate of events is $\lambda$ in a given time interval,then it can be shown that the interval time between two events is a continuous R.V. with exponential distribution.*

**Normal Distribution/Gaussion Distribution**

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}},x\in(-\infty,\infty),\sigma\in[0,\infty)$$

> 写作$X\sim(\mu,\sigma^{2})$  
> $\int_{-\infty}^{\infty}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}dx=\sqrt{2\pi}\sigma$

正态分布本身是一个美丽的分布，它不仅是世界上绝大多数规律的展示，同时他还有很多性质。

1. Let X be a normal R.V. of parameters $(\mu,\sigma^{2})$ and consider the new R.V. Y such that $Y=aX+b$
$$\Rightarrow\, E(Y)=a\mu +b,Var(Y)=a^{2}\sigma^{2}$$
$$Y\sim N(a\mu+b,a^{2}\sigma^{2})$$

> proof.  
> $P(Y\leq y)=P(aX+b\leq y)=P(X\leq \frac{y-b}{a})=F_{X}(\frac{y-b}{a})$  
> $\therefore F_{Y}(y)=F_{X}(\frac{y-b}{a})$  
> $f_{Y}(y)=\frac{dF_{Y}(y)}{dy}=\frac{dF_{X}(\frac{y-b}{a})}{dy}=f_{X}(\frac{y-b}{a})\cdot\frac{1}{a}$  
> $\therefore f_{Y}(y)=\frac{1}{a}\cdot\frac{1}{\sqrt{2\pi\sigma}}\cdot e^{-\frac{(\frac{y-b}{a}-\mu)^{2}}{2\sigma^{2}}}=\frac{1}{\sqrt{2\pi}\sigma a}\cdot e^{-\frac{(y-b-\mu a)^{2}}{2\sigma^{2}a^{2}}}$ 

2. Standardizing  
Let X a normal R.V. of mean $\mu$ and variance $\sigma^{2}$
$$Z=\frac{X-\mu}{\sigma}\sim N(0,1)是standard\,normal\,R.V. $$
$$P(a\leq X\leq b)=P(\frac{a-\mu}{\sigma}\leq\frac{X-\mu}{\sigma}\leq\frac{b-\mu}{\sigma})=\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma})$$

3. CDF of $X\sim N(0,1)$
$$\Phi(x)=P(X\leq x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-\frac{t^{2}}{2}}dt$$

> 对于$\Phi(x)$,有$\Phi(-x)=1-\Phi(x)$

4. 3$\sigma$原则
$$P(\mu-\sigma\leq X\leq \mu+\sigma)\approx0.68$$
$$P(\mu-2\sigma\leq X\leq \mu+2\sigma)\approx0.95$$
$$P(\mu-3\sigma\leq X\leq \mu+3\sigma)\approx0.997$$

这里的X是服从均值为$\mu$方差为$\sigma^{2}$的正态分布，因此通过标准化之后就可以查表得到结果。这个原则告诉我们一个正态分布的随机变量其取值在3个标准差之内的概率几乎就是1。因此如果出现了超出3$\sigma$的值，那么我们认为有问题。

5. Normal approximation to the Binomial distribution  
之前我们说过用泊松去近似二项分布，现在可以用正态分布去近似二项分布

$$As\,n\rightarrow\infty,with\,\mu=np\,\,and\,\, \sigma^{2}=np(1-p),X\sim Bin(n,p)$$
$$P(a\leq X\leq b)=P(\frac{a-np}{\sqrt{np(1-p)}}\leq\frac{X-np}{\sqrt{np(1-p)}}\leq\frac{b-np}{\sqrt{np(1-p)}})\approx\Phi()-\Phi()$$

用正态的话并不要求p很小，只要均值和方差是有限值即可。因此相对来说会比用泊松来近似的条件要求的少。

> * $E(x)=arg_{a}\,min\,E(X-a)^{2}$  
> * $m(x)=arg_{a}\,min\,E|X-a|$   
> The median of a continuous R.V. X of pdf f(x) is the number such that $\int_{-\infty}^{m}f(x)dx=\int_{m}^{\infty}f(x)dx=\frac{1}{2}\Leftrightarrow P(X\leq m)=P(X\geq m)=\frac{1}{2}$ 


##Other continuous distribution

**Log-normal Distribution**

X follow a log-normal $(\mu,\sigma)$ Distribution if its pdf
$$f_{X}(x)=\frac{1}{\sqrt{2\pi}\sigma}\frac{1}{x}e^{-\frac{(logX-\mu)^{2}}{2\sigma^{2}}},\quad x\in(0,\infty)$$
$$f_{X}(x)=0,\quad otherwise$$

> If X follows a log-normal$(\mu,\sigma)$,then $logX\sim N(\mu,\sigma^{2})$

**Gamma Distribution**

**Beta Distribution**

##Change of known variable

一般来说就是，给出X的pdf，然后要你求$Y=g(X)$的pdf

> 永远从CDF入手，先找$F_{Y}(y)=P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))$  
> 再运用$f(y)=\frac{dF_{Y}(y)}{dy}$求解


#Chapter 6 Jointly Distributed R.V.

这一章的逻辑顺序是：

1. 由*单个变量分布*进入到探讨*两个变量的联合分布*
2. *sum of independent R.V.s的分布*（这是一个单独的变量）
3. 两个变量（这两个变量是另外两个已知变量的函数）的联合分布
4. 条件分布

##Jointly Distributed R.V.

Assume we have two R.V. X and Y,then we define the joint CDF
$$F_{X,Y}(a,b)=P(X\leq a,Y\leq b)$$
$$F_{X}(a)=P(X\leq a,Y\leq\infty)=lim_{b\rightarrow\infty}F_{X,Y}(a,b)$$
$$F_{Y}(b)=P(X\leq\infty,Y\leq b)=lim_{a\rightarrow\infty}F(a,b)$$

For discrete variables,we work directly with the *joint pmf*
$$P(x,y)=P(X=x,Y=y)$$
from which we obtain
$$P_{X}(x)=\sum_{y}P(x,y),\quad P_{Y}(y)=\sum_{x}P(x,y)$$

> 如何理解这个joint？实际上就是用了“且”的表示方法，所以当我们想只要单个变量的时候我们就把另一个变量的所有取值可能都取了，就得到单个变量的了（这实际上就是贝叶斯公式那里的思想）。

If both X and Y are jointly continuous,then their *joint pdf* is a non-negative function f(x,y) such that for any set C
$$P\{(X,Y)\in C\}=\int\int_{(X,Y)\in C}f(x,y)dxdy$$

> 首先我们想象一下这个pdf，它不同于一元情况下的pdf，因为在一元下y=f(x)是一个二维空间，所以我们如果积分积出来的是面积，并且整个面积和为1；而现在是二元情况，那么这时候z=f(x,y)是一个三维空间，我们积分积出来的是体积，并且整个体积为1。  
> 你会发现我们永远都是先定义CDF再定义pdf，因为CDF具有概率的意义，而pdf只是对概率的刻画，它是概率密度，cdf与pdf是通过积分连接的，从而决定了F(x,y)与f(x,y)的原函数与导函数之间的关系。

In particular we have
$$F(a,b)=P(X\leq a,Y\leq b)=\int_{-\infty}^{a}\int_{-\infty}^{b}f(x,y)dydx$$

Hence,when we differentiate,we obtain
$$f(x,y)=\frac{\partial^{2}F(x,y)}{\partial x\partial y}$$

有了f(x,y)我们就能对一开始描述的cdf用积分来描述：
$$F_{X}(a)=lim_{b\rightarrow\infty}F(a,b)=\int_{-\infty}^{a}\int_{-\infty}^{\infty}f(x,y)dydx$$
$$f_{X}(x)=\infty_{-\infty}^{\infty}f(x,y)dy$$
$$f_{Y}(y)=\infty_{-\infty}^{\infty}f(x,y)dx$$

> 对于f(x)、f(y)的这种形式，可以理解为是变上限积分求导的结果。

##Independent R.V.

The R.V. X and Y are said to be independent if,for any sets A and B we have
$$P(x\in A,Y\in B)=P(X\in A)P(Y\in B)$$
$$\Leftrightarrow\quad F(x,y)=F_{X}(x)F_{Y}(y)$$
$$\Leftrightarrow\quad f(x,y)=f_{X}(x)f_{Y}(y)$$

> 独立带来的最直接也是最重要的效果就是：联合分布是边际分布的乘积


##Sum of Independent R.V.

我们其实就是要求出Z=X+Y或者再复杂一些的变量加减运算后得到的新变量的分布。

对于离散变量，求pmf。我们以Z=X+Y为例

因为当Z=z时，如果Y=y则必有X=z-y，且

$$P(X=z-y\cap Y=y)=P(X=z-y)P(Y=y)$$

又因为Y可以取其他值，每当Y取不同值，X也要对应变化
$$\Rightarrow P_{Z}(z)=\sum_{y=-\infty}^{\infty}P_{X}(z-y)P_{Y}(y)$$

对于连续变量求pdf，我们仍然以Z=X+Y为例

*对于求pdf，我们总是从CDF开始求起*

$$F_{Z}(z)=P(Z\leq z)=P(X+Y\leq z)=P(X\leq z-Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{z-y}f(x,y)dxdy=\int_{-\infty}^{\infty}\int_{-\infty}^{z-y}f(x)f(y)dxdy$$

$$\Rightarrow\quad F_{Z}(z)=\int_{-\infty}^{\infty}f_{Y}(y)\int_{-\infty}^{z-y}f_{X}(x)dxdy=\int_{-\infty}^{\infty}f_{Y}(y)F_{X}(z-y)dy$$
$$\therefore\quad f_{Z}(z)=\frac{dF_{Z}(z)}{dz}=\int_{-\infty}^{\infty}f_{Y}(y)f_{X}(z-y)dy$$

> 这个最终的式子有一个名字：卷积。卷积的定义——两个变量在某范围内相乘后求和的结果，它是z的函数，y被积分掉了。  
> 我们以后会经常使用卷积，主要就是用在这里。但是不要每次都一开始就把f()打开了带进去来求，而是直接使用上面这个式子即可。我想说明的是，这个式子总是正确的，不论f(x)、f(y)是怎样的。因为，在推导过程中我们得到的那个F(z-y)永远是对的，虽然我们写着是从负无穷积上来，但是因为F()是一个不递减的函数，所以积分区域选到多余的地方也没关系，多余的地方f()=0。我们所要讨论的是，在什么时候$f_{Y}(y)$、$f_{X}(z-y)$可以被打开，那么就对应取y的区间就好。

**sum of Poisson R.V.**

$X\sim Poisson(\lambda)$,$Y\sim Poisson(\lambda')$,Z=X+Y

$$P_{Z}(z)=\sum_{y=-\infty}^{\infty}P_{X}(z-y)P_{Y}(y)=\sum_{y=0}^{z}P_{X}(z-y)P_{Y}(y)\quad,因为(z-y)大于0且y大于0时才有正概率$$
$$\Rightarrow\quad P_{Z}(z)=\sum_{y=0}^{z}e^{-\lambda}\frac{\lambda^{z-y}}{(z-y)!}e^{-\lambda'}\frac{(\lambda')^{y}}{y!}=e^{-(\lambda+\lambda')}\sum_{y=0}^{z}\frac{\lambda^{z-y}\cdot(\lambda')^{y}}{(z-y)!y!}$$
$$\therefore\quad P_{Z}(z)=e^{-(\lambda+\lambda')}\frac{(\lambda+\lambda')^z}{z
!}$$

所以$Z\sim Poisson(\lambda+\lambda')$

**sum of Uniform R.V.**

$f_{X}(x)=1_{[0,1]}(x)$,$f_{Y}(y)=1_{[0,1]}(y)$,Z=X+Y

$$P(Z\leq z)=P(X\leq z-Y)=\int_{-\infty}^{\infty}f_{X}(z-y)f_{Y}(y)dy=\int_{0}^{1}f_{X}(z-y)dy$$

现在的问题就是什么时候我们可以打开这个f()，所以我们必须判断(z-y)的值什么时候能够在[0,1]中。

$$当Z\in [0,1],\quad 0\leq z-y\leq 1\quad\Rightarrow\quad z-1\leq y\leq z$$
$$\int_{0}^{1}f_{X}(z-y)dy=\int_{0}^{z}dy=z$$
$$当Z\in (1,2],\quad 0\leq z-y\leq 1\quad\Rightarrow\quad z-1\leq y\leq z$$
$$\int_{0}^{1}f_{X}(z-y)dy=\int_{z-1}^{1}dy=2-z$$

**sum of Exponential R.V.**

Consider two independent exponential R.V. X,Y of parameter $\lambda$,Z=X+Y

For $z<0$,$f_{Z}(z)=0$,for $z>0$

$$f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(z-y)f_{Y}(y)dy=\int_{0}^{\infty}f_{X}(z-y)\lambda e^{-\lambda x}dy=\int_{0}^{z}\lambda e^{-\lambda(z-y)}\lambda e^{-\lambda x}=\lambda^{2}z\cdot e^{-\lambda z}$$

> 可以看到，我们分两次打开了f()，两次分别把y的取值压缩到(0,z)

**sum of Gaussian R.V.**

First consider two standard normal R.V. X and Y,Z=X+Y

$$f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(z-y)f_{Y}(y)dy=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{\frac{(z-y)^{2}}{2}}\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}=\frac{1}{\sqrt{2\pi}\sqrt{2}}e^{-\frac{z^2}{4}}$$
$$\therefore\quad Z\sim N(0,2)$$

Generalization:If X is an normal R.V.$(\mu_{x},\sigma_{x}^{2})$ and Y is an normal R.V.$(\mu_{y},\sigma_{y}^{2})$ where X and Y are independent then Z is a normal R.V.$(\mu_{x}+\mu_{y},\sigma_{x}^{2}+\sigma_{y}^{2})$

##Jointly probability distribution of functions of R.V.

我们考虑这样一种情况，需求U和供给V都会受到商品价格X和消费者收入Y的影响，也就是说$U=g_{1}(X,Y),V=g_{2}(X,Y)$.而我们希望找到(U,V)的联合分布。已知(X,Y)的联合分布。

**Definition:Jacobian matrix and Jacobian**

Consider the bivariate transformation:$U=g_{1}(X,Y)$,$V=g_{2}(X,Y)$,where $g_{1}$,$g_{2}$ are continuously differentiable with respect to X,Y.Then the $2\times2$ matrix

$$J_{UV}(x,y)=
 \left(
 \begin{array}{11}
  \frac{\partial g_{1}(x,y)}{\partial x} & \frac{\partial g_{1}(x,y)}{\partial y} \\
  \frac{\partial g_{2}(x,y)}{\partial x} & \frac{\partial g_{2}(x,y)}{\partial y}
  \end{array}
  \right)
$$

is called the *Jacobian matrix* of (U,V).It's determinant is called the *Jacobian*.

**Inverse Function**

简单来说就是$U=g_{1}(X,Y)$,$V=g_{2}(X,Y)$.则反向表示$X=h_{1}(U,V)$,$Y=h_{2}(U,V)$.我们把$(h_{1},h_{2})$叫做inverse functions of $(g_{1},g_{2})$

**Theorem**

$$det[J_{XY}(U,V)]=\frac{1}{det[J_{UV}(X,Y)]}$$

**Theorem:Bivariate Transformation**

We have $A=\{(x,y)|f_{XY}(x,y)>0\}$,$B=\{(u,v)|f_{UV}(u,v)>0\}$,要求A和B是一一对应的，且$g_{1},g_{2}$是连续可导的，$det[J_{uv}(X,Y)]\neq0$,则
$$f_{UV}(u,v)=
 \left\{
 \begin{array}{11}
 \frac{f_{XY}(h_{1}(u,v),h_{2}(u,v))}{|det[J_{U,V}(x,y)]|} & for (u,v)\in B\\
 0 & otherwise\\
 \end{array}
 \right.
 $$
 
##Conditional Distributions

**Discrete Case**

$$P_{X|Y}(x|y)=\frac{P_{XY}(x,y)}{P_{Y}(y)}=\frac{P_{Y|X}(y|x)P_{X}(x)}{P_{Y}(y)}$$
$$E(X|Y=y)=\sum_{x}x\cdot P_{X|Y}(x|y),\quad Var(X|Y=y)=E(X^{2}|Y=y)-[E(X|Y=y)]^{2}$$

*$E(X|Y=y)$是一个function，但是已经没有随机性了！而$E(X|Y)$是一个R.V.具有随机性，随机性来自于Y而不是X，因为经过期望之后其实就是经过了积分，X已经被积分掉了。*

$$E_{Y}(E_{X}(X|Y))=E(X)$$

**Continuous Case**

$$f_{X|Y}(x|y)=\frac{f(x,y)}{f(y)}=\frac{f_{Y|X}(y|x)f_{X}(x)}{f_{Y}(y)}$$
$$E(X|Y=y)=\int x\cdot f_{X|Y}(x|y)dx,\quad Var(X|Y=y)=E(X^{2}|Y=y)-[E(X|Y=y)]^{2}$$

##Order Statistic

有一系列的iid的R.V. $X_{i}$，我们对它们进行排序:
$$X_{(1)}<X_{(2)}<X_{(3)}<...<X_{(n)}$$
$$X_{(1)}=smallest\,\,of\,\,X_{1},X_{2},X_{3},...,X_{n}$$
$$X_{(n)}=largest\,\,of\,\,X_{1},X_{2},X_{3},...,X_{n}$$
则我们可以得到一些分布
$$f_{X_{(1)},X_{(2)},...,X_{(n)}}(x_{1},x_{2},...,x_{n})=n!\cdot f(x_{1})f(x_{2})\cdots f(x_{n})$$
$$Range\{X_{1},X_{2},...,X_{n}\}= X_{(n)}-X_{(1)}$$

对于最小统计量
$$F_{X_{(1)}}(x)=P(X_{(1)}\leq x)=1-P(\textrm{all the }X_{i}\geq x)=1-P(X_{1}\geq x,X_{2}\geq x,...,X_{n}\geq x)=1-[1-F(x)]^{n}$$
$$\Rightarrow\quad f_{X_{(1)}}(x)=n\cdot[1-F(x)]^{n-1}f(x)$$

对于最大统计量
$$F_{X_{(n)}}(x)=P(X_{(n)}\leq x)=P(\textrm{all }X_{i}\leq x)=[F(x)]^{n}$$
$$\Rightarrow\quad f_{X_{(n)}}(x)=n\cdot[F(x)]^{n-1}\cdot f(x)$$

对于任意一个统计量
$$F_{X_{(i)}}(x)=P(X_{(i)}\leq x)=P(\textrm{at least i 个 }X\leq x)=\sum_{m=i}^{n}P(\textrm{exactly m 个 }X\leq x)=\sum_{m=i}^{n}C_{n}^{m}(F(x))^{m}[1-F(x)]^{n-m}$$
$$f_{X_{(i)}}(x)=f(x)\sum_{m=i}^{n}C_{n}^{m}[F(x)]^{m-1}[1-F(x)]^{n-m-1}[m-nF(x)]$$

#Chapter 7 Properties of Expectation


 